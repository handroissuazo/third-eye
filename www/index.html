<!DOCTYPE HTML>
<html lang="en-US">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/styles.css">
    <title>Third Eye</title>
</head>
<body>
<nav></nav>
<section id="MainContent" class="container-fluid">
    <div class="row">
        <div class="col-md-12">
            <h1>Third Eye</h1>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/CC65FuF6MCc" frameborder="0" allowfullscreen></iframe>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Point Cloud Generation Breakdown</h3>
        </div>
        <div class="col-md-4">
            <h4>High-Level Workflow</h4>
            <ul>
                <li>
                    <b>Server</b>
                    <p>
                        Detectes when new sample video has been recorded by drone begins the processing stage.
                    </p>
                </li>
                <li>
                    <b>FFMPEG</b>
                    <p>
                        samples videos for images required by VisualSFM.
                    </p>
                </li>
                <li>
                    <b>VisualSFM</b>
                    <p>
                        Performes 'structure-from-motion' calculations to generate a point cloud representation of the target.
                    </p>
                </li>
            </ul>
        </div>
        <div class="col-md-4">
            <h4>Technical</h4>
            <ul>
                <li>
                    <b>SIFT/PBA Scale-invariant feature transform</b>
                    <p>
                        We first run our photos through a sift algorithm to detect local features in our images.
                    This analysis determines depth, scale, and orientation based on the relational position between the local features or points between images, thus creating a vector matrix for each point'sâ€™ movement from one picture to the next.    
                    </p>
                    <p>
                        The results of this filtering, named the sparse reconstruction, serve as the starting points for the following analytical algorithms.     
                    </p>
                </li>
                <li>
                    <b>CMVS  Clustering Views for Multi-view Stereo</b>
                    <p>
                        This algorithm attempts to construct a three dimensional image based on the vector and positional information generated by the SIFT algorithm. It takes sparse reconstruction and estimates the orientation of each point on a three dimensional plane using the relational movement data.
                    </p>
                    <p>
                        The results of this analysis is an actual point cloud, named the dense reconstruction, which can be viewed in most CAD softwares.
                    </p>
                </li>
            </ul>
        </div>
        <div class="col-md-4">
            <h4>Caveats</h4>
            <ul>
                <li>
                    <b>Hardware Limitations</b>
                    <p>
                        We require at least 30 seconds of smooth dynamic camera footage of an object to create an accurate representation. 
                    </p>
                </li>
                <li>
                    <b>Legal Limitations</b>
                    <p>
                        When flying  a drone, according to FAA guidelines, must be a considerable and safe distance from people and objects. This restriction only allows us to capture outdoor footage with the drone.
                    </p>
                </li>
                <li>
                    <b>Software Limitations</b>
                    <p>
                        The algorithms and analysis are quite slow. We must leave 30 to 45 minute post render times for our captures.
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Bibliography</h3>
            <ul>
                <li>
                    <a href="http://www.di.ens.fr/cmvs/">Furukawa, Yasutaka. "CMVS." CMVS. University of Washington, 17 July 2010. Web. 30 Sept. 2016.</a>
                </li>
                <li>
                    <a href="http://www.di.ens.fr/pmvs/">Furuwaka, Yasutaka, and Jean Ponce. "PMVS2." PMVS2. University of Illinois, 13 July 2010. Web. 15 Sept. 2016. </a>
                </li>
                <li>
                    <a href="http://grail.cs.washington.edu/projects/mcba">Wu, Changchang, Sameer Agarwal, Brian Curless, and Steven M. Seitz. "Multicore Bundle Adjustment." Multicore Bundle Adjustment. University of Washington, 2011. Web. 20 Sept. 2016. </>.</a>
                </li>
                <li>
                    <a href="http://ccwu.me/vsfm/vsfm.pdf">Wu, Changchang. "Towards Linear-time Incremental Structure from Motion." VisualSFM : A Visual Structure from Motion System. University of Washington, n.d. Web. 16 Sept. 2016.</a>
                </li>
                <li>
                    <a href="http://www.python.org"></a>Python Software Foundation. Python Language Reference, version 3.5.
                </li>
            </ul>
        </div>
    </div>
</section>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
</body>
</html>